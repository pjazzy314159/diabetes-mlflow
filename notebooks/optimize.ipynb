{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a252cf9e",
   "metadata": {},
   "source": [
    "### Optimizing based on the main.ipynb\n",
    "- Adding hyperparameter tuning.\n",
    "- Changing ROC-AUC to PR-AUC due to imbalanced data.\n",
    "- Tuning threshold to determine better cut off.\n",
    "- Optimizing F2-score instead of F1-score because in this case, recall is more important than precision.\n",
    "- Changing KFold to StratifiedKFold to keep the ratio of classes in each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8ac8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tandu\\.conda\\envs\\diabetes\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    average_precision_score, precision_recall_curve, classification_report,\n",
    "    confusion_matrix, fbeta_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c23a852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/10 15:40:36 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2026/01/10 15:40:36 INFO mlflow.store.db.utils: Updating database tables\n",
      "2026/01/10 15:40:36 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/10 15:40:36 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/01/10 15:40:36 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/10 15:40:36 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/01/10 15:40:36 INFO mlflow.tracking.fluent: Experiment with name 'Diabetes Prediction - Stacking PR-AUC' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE enabled: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Config\n",
    "THRESHOLD = 0.3  \n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "USE_SMOTE = True  # Handle imbalanced data\n",
    "\n",
    "# MLflow setup\n",
    "mlflow.set_tracking_uri('sqlite:///mlflow.db')\n",
    "mlflow.set_experiment(\"Diabetes Prediction - Stacking PR-AUC\")\n",
    "\n",
    "# SMOTE config\n",
    "smote = SMOTE(random_state=RANDOM_STATE, sampling_strategy='minority')\n",
    "print(f\"SMOTE enabled: {USE_SMOTE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d9ed94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (324372, 21)\n",
      "Class distribution:\n",
      "Diabetes_binary\n",
      "0.0    0.767788\n",
      "1.0    0.232212\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load and preprocess data\n",
    "df1 = pd.read_csv('../data/db1.csv')\n",
    "df2 = pd.read_csv('../data/db2.csv')\n",
    "\n",
    "df1['Diabetes_binary'] = df1['Diabetes_binary'].replace({2: 1})\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "X = df.drop('Diabetes_binary', axis=1)\n",
    "y = df['Diabetes_binary']\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Class distribution:\\n{y.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca5ab7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Helper functions\n",
    "def predict_with_threshold(model, X, threshold=0.5):\n",
    "    \"\"\"Predict with custom threshold\"\"\"\n",
    "    proba = model.predict_proba(X)[:, 1]\n",
    "    return (proba >= threshold).astype(int), proba\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_proba):\n",
    "    \"\"\"Calculate metrics including PR-AUC\"\"\"\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'pr_auc': average_precision_score(y_true, y_proba),  # PR-AUC\n",
    "    }\n",
    "\n",
    "def find_optimal_threshold(y_true, y_proba, target_recall=0.7):\n",
    "    \"\"\"Find optimal threshold to achieve target recall\"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "    \n",
    "    # Find threshold with recall >= target_recall and highest precision\n",
    "    valid_idx = np.where(recall[:-1] >= target_recall)[0]\n",
    "    if len(valid_idx) == 0:\n",
    "        return 0.5\n",
    "    \n",
    "    best_idx = valid_idx[np.argmax(precision[:-1][valid_idx])]\n",
    "    return thresholds[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adee23b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "OPTUNA HYPERPARAMETER TUNING FOR BASE MODELS\n",
      "============================================================\n",
      "\n",
      "üîç Tuning XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 24. Best value: 0.712748: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [11:09<00:00, 22.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ XGBoost Best F2: 0.7127\n",
      "\n",
      "üîç Tuning LightGBM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 18. Best value: 0.707708: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [06:38<00:00, 13.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LightGBM Best F2: 0.7077\n",
      "\n",
      "üîç Tuning CatBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: 0.708712: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [33:04<00:00, 66.15s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CatBoost Best F2: 0.7087\n",
      "\n",
      "============================================================\n",
      "BEST HYPERPARAMETERS SUMMARY\n",
      "============================================================\n",
      "\n",
      "XGBoost: {'n_estimators': 78, 'max_depth': 9, 'learning_rate': 0.02603524973787783, 'subsample': 0.6994347093531768, 'colsample_bytree': 0.6621048297242669, 'min_child_weight': 9, 'gamma': 1.5069904782784755}\n",
      "\n",
      "LightGBM: {'n_estimators': 162, 'max_depth': 3, 'learning_rate': 0.03080147856778732, 'subsample': 0.6642164054514699, 'colsample_bytree': 0.8381282826211935, 'min_child_samples': 39, 'reg_alpha': 0.0014561469589187425, 'reg_lambda': 0.018581525458581882}\n",
      "\n",
      "CatBoost: {'iterations': 58, 'depth': 10, 'learning_rate': 0.024112898115291985, 'l2_leaf_reg': 0.009176996354542699, 'bagging_temperature': 3.1171107608941098, 'random_strength': 5.200680211778108}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    " # Cell 5: Optuna Hyperparameter Tuning for Base Models\n",
    "print(\"=\" * 60)\n",
    "print(\"OPTUNA HYPERPARAMETER TUNING FOR BASE MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "N_TRIALS_BASE = 30  # S·ªë trials cho m·ªói base model\n",
    "\n",
    "# ============ XGBoost Tuning ============\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'eval_metric': 'aucpr',\n",
    "        'use_label_encoder': False,\n",
    "    }\n",
    "    \n",
    "    skf_tune = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf_tune.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        if USE_SMOTE:\n",
    "            X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        proba = model.predict_proba(X_val)[:, 1]\n",
    "        pred = (proba >= THRESHOLD).astype(int)\n",
    "        f2 = fbeta_score(y_val, pred, beta=2)\n",
    "        scores.append(f2)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "print(\"\\nüîç Tuning XGBoost...\")\n",
    "study_xgb = optuna.create_study(direction='maximize', sampler=TPESampler(seed=RANDOM_STATE))\n",
    "study_xgb.optimize(objective_xgb, n_trials=N_TRIALS_BASE, show_progress_bar=True)\n",
    "best_xgb_params = study_xgb.best_params\n",
    "print(f\"‚úÖ XGBoost Best F2: {study_xgb.best_value:.4f}\")\n",
    "\n",
    "# ============ LightGBM Tuning ============\n",
    "def objective_lgbm(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'verbose': -1,\n",
    "    }\n",
    "    \n",
    "    skf_tune = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf_tune.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        if USE_SMOTE:\n",
    "            X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        model = LGBMClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        proba = model.predict_proba(X_val)[:, 1]\n",
    "        pred = (proba >= THRESHOLD).astype(int)\n",
    "        f2 = fbeta_score(y_val, pred, beta=2)\n",
    "        scores.append(f2)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "print(\"\\nüîç Tuning LightGBM...\")\n",
    "study_lgbm = optuna.create_study(direction='maximize', sampler=TPESampler(seed=RANDOM_STATE))\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=N_TRIALS_BASE, show_progress_bar=True)\n",
    "best_lgbm_params = study_lgbm.best_params\n",
    "print(f\"‚úÖ LightGBM Best F2: {study_lgbm.best_value:.4f}\")\n",
    "\n",
    "# ============ CatBoost Tuning ============\n",
    "def objective_catboost(trial):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 50, 300),\n",
    "        'depth': trial.suggest_int('depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 10),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0, 10),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'verbose': 0,\n",
    "        'eval_metric': 'PRAUC',\n",
    "    }\n",
    "    \n",
    "    skf_tune = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf_tune.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        if USE_SMOTE:\n",
    "            X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        proba = model.predict_proba(X_val)[:, 1]\n",
    "        pred = (proba >= THRESHOLD).astype(int)\n",
    "        f2 = fbeta_score(y_val, pred, beta=2)\n",
    "        scores.append(f2)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "print(\"\\nüîç Tuning CatBoost...\")\n",
    "study_catboost = optuna.create_study(direction='maximize', sampler=TPESampler(seed=RANDOM_STATE))\n",
    "study_catboost.optimize(objective_catboost, n_trials=N_TRIALS_BASE, show_progress_bar=True)\n",
    "best_catboost_params = study_catboost.best_params\n",
    "print(f\"‚úÖ CatBoost Best F2: {study_catboost.best_value:.4f}\")\n",
    "\n",
    "# ============ Summary ============\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BEST HYPERPARAMETERS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nXGBoost: {best_xgb_params}\")\n",
    "print(f\"\\nLightGBM: {best_lgbm_params}\")\n",
    "print(f\"\\nCatBoost: {best_catboost_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c129d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CREATING BASE MODELS WITH TUNED HYPERPARAMETERS\n",
      "============================================================\n",
      "\n",
      "‚úÖ Base models created with tuned hyperparameters:\n",
      "  - XGB\n",
      "  - LGBM\n",
      "  - CATBOOST\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Create Base Models with Best Hyperparameters\n",
    "print(\"=\" * 60)\n",
    "print(\"CREATING BASE MODELS WITH TUNED HYPERPARAMETERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Build base models with best params from Optuna\n",
    "base_models = {\n",
    "    'xgb': XGBClassifier(\n",
    "        **best_xgb_params,\n",
    "        random_state=RANDOM_STATE,\n",
    "        eval_metric='aucpr',\n",
    "        use_label_encoder=False\n",
    "    ),\n",
    "    'lgbm': LGBMClassifier(\n",
    "        **best_lgbm_params,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=-1\n",
    "    ),\n",
    "    'catboost': CatBoostClassifier(\n",
    "        **best_catboost_params,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=0,\n",
    "        eval_metric='PRAUC'\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Base models created with tuned hyperparameters:\")\n",
    "for name, model in base_models.items():\n",
    "    print(f\"  - {name.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f366b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STACKING WITH PR-AUC (Threshold = 0.3)\n",
      "SMOTE: Enabled\n",
      "============================================================\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "  SMOTE: 259497 ‚Üí 398478 samples\n",
      "  xgb: PR-AUC=0.5802, Recall=0.8945, Precision=0.3952\n",
      "  lgbm: PR-AUC=0.5718, Recall=0.8907, Precision=0.3890\n",
      "  catboost: PR-AUC=0.5724, Recall=0.9101, Precision=0.3782\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "  SMOTE: 259497 ‚Üí 398478 samples\n",
      "  xgb: PR-AUC=0.5675, Recall=0.8947, Precision=0.3951\n",
      "  lgbm: PR-AUC=0.5588, Recall=0.8927, Precision=0.3881\n",
      "  catboost: PR-AUC=0.5588, Recall=0.9105, Precision=0.3771\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "  SMOTE: 259498 ‚Üí 398478 samples\n",
      "  xgb: PR-AUC=0.5718, Recall=0.8897, Precision=0.3970\n",
      "  lgbm: PR-AUC=0.5647, Recall=0.8866, Precision=0.3905\n",
      "  catboost: PR-AUC=0.5632, Recall=0.9051, Precision=0.3776\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "  SMOTE: 259498 ‚Üí 398478 samples\n",
      "  xgb: PR-AUC=0.5748, Recall=0.8941, Precision=0.3950\n",
      "  lgbm: PR-AUC=0.5681, Recall=0.8937, Precision=0.3892\n",
      "  catboost: PR-AUC=0.5686, Recall=0.9100, Precision=0.3780\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "  SMOTE: 259498 ‚Üí 398480 samples\n",
      "  xgb: PR-AUC=0.5719, Recall=0.8902, Precision=0.3953\n",
      "  lgbm: PR-AUC=0.5637, Recall=0.8893, Precision=0.3883\n",
      "  catboost: PR-AUC=0.5661, Recall=0.9047, Precision=0.3781\n",
      "\n",
      "============================================================\n",
      "AVERAGE METRICS (K-Fold)\n",
      "============================================================\n",
      "\n",
      "XGB:\n",
      "  accuracy: 0.6583\n",
      "  precision: 0.3955\n",
      "  recall: 0.8926\n",
      "  f1: 0.5482\n",
      "  pr_auc: 0.5732\n",
      "\n",
      "LGBM:\n",
      "  accuracy: 0.6498\n",
      "  precision: 0.3890\n",
      "  recall: 0.8906\n",
      "  f1: 0.5415\n",
      "  pr_auc: 0.5654\n",
      "\n",
      "CATBOOST:\n",
      "  accuracy: 0.6314\n",
      "  precision: 0.3778\n",
      "  recall: 0.9081\n",
      "  f1: 0.5336\n",
      "  pr_auc: 0.5658\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: K-Fold Cross Validation v·ªõi Stacking + SMOTE\n",
    "from sklearn.base import clone\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Storage cho meta features\n",
    "oof_predictions = {name: np.zeros(len(X)) for name in base_models}\n",
    "fold_metrics = {name: [] for name in base_models}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"STACKING WITH PR-AUC (Threshold = {THRESHOLD})\")\n",
    "print(f\"SMOTE: {'Enabled' if USE_SMOTE else 'Disabled'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with mlflow.start_run(run_name=\"stacking_kfold_prauc\"):\n",
    "    mlflow.log_param(\"n_folds\", N_FOLDS)\n",
    "    mlflow.log_param(\"threshold\", THRESHOLD)\n",
    "    mlflow.log_param(\"metric\", \"PR-AUC\")\n",
    "    mlflow.log_param(\"use_smote\", USE_SMOTE)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"\\n--- Fold {fold + 1}/{N_FOLDS} ---\")\n",
    "        \n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Apply SMOTE only on training data\n",
    "        if USE_SMOTE:\n",
    "            X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "            print(f\"  SMOTE: {len(y_train)} ‚Üí {len(y_train_resampled)} samples\")\n",
    "        else:\n",
    "            X_train_resampled, y_train_resampled = X_train, y_train\n",
    "        \n",
    "        for name, model in base_models.items():\n",
    "            model_clone = clone(model)\n",
    "            model_clone.fit(X_train_resampled, y_train_resampled)\n",
    "            \n",
    "            # Predict on ORIGINAL validation set (kh√¥ng SMOTE)\n",
    "            y_pred, y_proba = predict_with_threshold(model_clone, X_val, THRESHOLD)\n",
    "            \n",
    "            # Save OOF predictions (probabilities)\n",
    "            oof_predictions[name][val_idx] = y_proba\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = calculate_metrics(y_val, y_pred, y_proba)\n",
    "            fold_metrics[name].append(metrics)\n",
    "            \n",
    "            print(f\"  {name}: PR-AUC={metrics['pr_auc']:.4f}, \"\n",
    "                  f\"Recall={metrics['recall']:.4f}, Precision={metrics['precision']:.4f}\")\n",
    "    \n",
    "    # Log average metrics per model\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"AVERAGE METRICS (K-Fold)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for name in base_models:\n",
    "        avg_metrics = {\n",
    "            k: np.mean([m[k] for m in fold_metrics[name]]) \n",
    "            for k in fold_metrics[name][0]\n",
    "        }\n",
    "        print(f\"\\n{name.upper()}:\")\n",
    "        for k, v in avg_metrics.items():\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "            mlflow.log_metric(f\"{name}_{k}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee6038a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OPTUNA HYPERPARAMETER TUNING FOR META LEARNER\n",
      "============================================================\n",
      "Meta features shape: (324372, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 25. Best value: 0.690257: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [02:53<00:00,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Best F2-Score: 0.6903\n",
      "Best Threshold: 0.201\n",
      "Best Params: {'C': 1.973072411270085, 'penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Optuna Hyperparameter Tuning for Meta Learner\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OPTUNA HYPERPARAMETER TUNING FOR META LEARNER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create meta features from OOF predictions\n",
    "meta_features = np.column_stack([oof_predictions[name] for name in base_models])\n",
    "meta_features_df = pd.DataFrame(\n",
    "    meta_features, \n",
    "    columns=['xgb_pred', 'lgbm_pred', 'catboost_pred']\n",
    ")\n",
    "meta_features_df['true_label'] = y.values\n",
    "\n",
    "print(f\"Meta features shape: {meta_features.shape}\")\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function for meta learner tuning\"\"\"\n",
    "    \n",
    "    # Hyperparameters to tune\n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 0.001, 100, log=True),\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),\n",
    "        'solver': 'saga',  # Supports both l1 and l2\n",
    "        'max_iter': 2000,\n",
    "        'random_state': RANDOM_STATE,\n",
    "    }\n",
    "    \n",
    "    # Threshold tuning\n",
    "    threshold = trial.suggest_float('threshold', 0.2, 0.5)\n",
    "    \n",
    "    # Cross-validation for meta learner\n",
    "    skf_meta = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf_meta.split(meta_features, y):\n",
    "        X_train_meta = meta_features[train_idx]\n",
    "        X_val_meta = meta_features[val_idx]\n",
    "        y_train_meta = y.iloc[train_idx]\n",
    "        y_val_meta = y.iloc[val_idx]\n",
    "        \n",
    "        model = LogisticRegression(**params)\n",
    "        model.fit(X_train_meta, y_train_meta)\n",
    "        \n",
    "        # Predict with threshold\n",
    "        proba = model.predict_proba(X_val_meta)[:, 1]\n",
    "        pred = (proba >= threshold).astype(int)\n",
    "        \n",
    "        # Optimize F2-score (recall more important)\n",
    "        f2 = fbeta_score(y_val_meta, pred, beta=2)\n",
    "        scores.append(f2)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Run Optuna study\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=TPESampler(seed=RANDOM_STATE),\n",
    "    study_name='meta_learner_tuning'\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Best parameters\n",
    "best_params = study.best_params\n",
    "best_threshold = best_params.pop('threshold')\n",
    "\n",
    "print(f\"\\n‚úÖ Best F2-Score: {study.best_value:.4f}\")\n",
    "print(f\"Best Threshold: {best_threshold:.3f}\")\n",
    "print(f\"Best Params: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "933dce86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING META LEARNER WITH BEST PARAMS\n",
      "============================================================\n",
      "Using params: {'C': 1.973072411270085, 'penalty': 'l2', 'solver': 'saga', 'max_iter': 2000, 'random_state': 42}\n",
      "Using threshold: 0.201\n",
      "\n",
      "META LEARNER RESULTS (Threshold = 0.201):\n",
      "  accuracy: 0.7287\n",
      "  precision: 0.4522\n",
      "  recall: 0.7950\n",
      "  f1: 0.5765\n",
      "  pr_auc: 0.5736\n",
      "  f2: 0.6903\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Train Meta Learner with Best Params\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING META LEARNER WITH BEST PARAMS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use best params from Optuna\n",
    "meta_learner_params = {\n",
    "    'C': best_params['C'],\n",
    "    'penalty': best_params['penalty'],\n",
    "    'solver': 'saga',\n",
    "    'max_iter': 2000,\n",
    "    'random_state': RANDOM_STATE,\n",
    "}\n",
    "\n",
    "print(f\"Using params: {meta_learner_params}\")\n",
    "print(f\"Using threshold: {best_threshold:.3f}\")\n",
    "\n",
    "# Train meta learner\n",
    "meta_learner = LogisticRegression(**meta_learner_params)\n",
    "meta_learner.fit(meta_features, y)\n",
    "\n",
    "# Predict v·ªõi best threshold\n",
    "meta_proba = meta_learner.predict_proba(meta_features)[:, 1]\n",
    "meta_pred = (meta_proba >= best_threshold).astype(int)\n",
    "\n",
    "# T√≠nh metrics cho meta learner\n",
    "meta_metrics = calculate_metrics(y, meta_pred, meta_proba)\n",
    "meta_metrics['f2'] = fbeta_score(y, meta_pred, beta=2)\n",
    "\n",
    "print(f\"\\nMETA LEARNER RESULTS (Threshold = {best_threshold:.3f}):\")\n",
    "for k, v in meta_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dccfb9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL TRAINING ON FULL DATA\n",
      "============================================================\n",
      "SMOTE applied: 324372 ‚Üí 498098 samples\n",
      "Class distribution after SMOTE:\n",
      "Diabetes_binary\n",
      "0.0    249049\n",
      "1.0    249049\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/10 16:36:41 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/01/10 16:36:54 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/01/10 16:37:15 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL STACKING RESULTS (Threshold = 0.201):\n",
      "  accuracy: 0.7355\n",
      "  precision: 0.4600\n",
      "  recall: 0.8004\n",
      "  f1: 0.5843\n",
      "  pr_auc: 0.5858\n",
      "  f2: 0.6972\n",
      "\n",
      "Confusion Matrix:\n",
      "[[178291  70758]\n",
      " [ 15037  60286]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.72      0.81    249049\n",
      "         1.0       0.46      0.80      0.58     75323\n",
      "\n",
      "    accuracy                           0.74    324372\n",
      "   macro avg       0.69      0.76      0.70    324372\n",
      "weighted avg       0.81      0.74      0.75    324372\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/10 16:37:27 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ All models and artifacts logged successfully!\n",
      "   Saved to ../data/:\n",
      "   - data_full.csv\n",
      "   - feature_names.csv\n",
      "   - meta_features.csv\n",
      "   - optuna_results.csv (all 4 models)\n",
      "   - config.json\n",
      "   MLflow models:\n",
      "   - base_model_xgb, base_model_lgbm, base_model_catboost\n",
      "   - meta_learner_final\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Final Training on Full Data with SMOTE\n",
    "import os\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL TRAINING ON FULL DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create data folder if not exists\n",
    "os.makedirs(\"../data\", exist_ok=True)\n",
    "\n",
    "with mlflow.start_run(run_name=\"stacking_full_data_optuna\"):\n",
    "    # Log params\n",
    "    mlflow.log_param(\"training_type\", \"full_data\")\n",
    "    mlflow.log_param(\"threshold\", best_threshold)\n",
    "    mlflow.log_param(\"metric\", \"PR-AUC\")\n",
    "    mlflow.log_param(\"meta_learner\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"base_models\", list(base_models.keys()))\n",
    "    mlflow.log_param(\"optuna_best_f2_meta\", study.best_value)\n",
    "    mlflow.log_param(\"optuna_best_f2_xgb\", study_xgb.best_value)\n",
    "    mlflow.log_param(\"optuna_best_f2_lgbm\", study_lgbm.best_value)\n",
    "    mlflow.log_param(\"optuna_best_f2_catboost\", study_catboost.best_value)\n",
    "    mlflow.log_param(\"n_samples\", len(X))\n",
    "    mlflow.log_param(\"n_features\", X.shape[1])\n",
    "    mlflow.log_param(\"use_smote\", USE_SMOTE)\n",
    "    \n",
    "    # Log Optuna best params for each model\n",
    "    for k, v in best_xgb_params.items():\n",
    "        mlflow.log_param(f\"xgb_{k}\", v)\n",
    "    for k, v in best_lgbm_params.items():\n",
    "        mlflow.log_param(f\"lgbm_{k}\", v)\n",
    "    for k, v in best_catboost_params.items():\n",
    "        mlflow.log_param(f\"catboost_{k}\", v)\n",
    "    for k, v in best_params.items():\n",
    "        mlflow.log_param(f\"meta_{k}\", v)\n",
    "    \n",
    "    # Apply SMOTE on full training data\n",
    "    if USE_SMOTE:\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "        print(f\"SMOTE applied: {len(y)} ‚Üí {len(y_resampled)} samples\")\n",
    "        print(f\"Class distribution after SMOTE:\\n{pd.Series(y_resampled).value_counts()}\")\n",
    "    else:\n",
    "        X_resampled, y_resampled = X, y\n",
    "    \n",
    "    # Train final base models on SMOTE data\n",
    "    final_base_models = {}\n",
    "    final_predictions = {}\n",
    "    \n",
    "    for name, model in base_models.items():\n",
    "        final_model = clone(model)\n",
    "        final_model.fit(X_resampled, y_resampled)\n",
    "        final_base_models[name] = final_model\n",
    "        \n",
    "        # Get predictions on ORIGINAL data (for evaluation)\n",
    "        proba = final_model.predict_proba(X)[:, 1]\n",
    "        final_predictions[name] = proba\n",
    "        \n",
    "        # Log base model\n",
    "        mlflow.sklearn.log_model(final_model, f\"base_model_{name}\")\n",
    "    \n",
    "    # Create final meta features (from original data predictions)\n",
    "    final_meta_features = np.column_stack([final_predictions[name] for name in base_models])\n",
    "    \n",
    "    # Train final meta learner with best params\n",
    "    final_meta_learner = LogisticRegression(**meta_learner_params)\n",
    "    final_meta_learner.fit(final_meta_features, y)\n",
    "    \n",
    "    # Final predictions v·ªõi best threshold\n",
    "    final_proba = final_meta_learner.predict_proba(final_meta_features)[:, 1]\n",
    "    final_pred = (final_proba >= best_threshold).astype(int)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    final_metrics = calculate_metrics(y, final_pred, final_proba)\n",
    "    final_metrics['f2'] = fbeta_score(y, final_pred, beta=2)\n",
    "    \n",
    "    print(f\"\\nFINAL STACKING RESULTS (Threshold = {best_threshold:.3f}):\")\n",
    "    for k, v in final_metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "        mlflow.log_metric(f\"overall_{k}\", v)\n",
    "    \n",
    "    # Log confusion matrix\n",
    "    cm = confusion_matrix(y, final_pred)\n",
    "    print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "    \n",
    "    # Log classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y, final_pred))\n",
    "    \n",
    "    # ========== LOG ARTIFACTS (save to ../data/) ==========\n",
    "    \n",
    "    # 1. Log raw data\n",
    "    df.to_csv(\"../data/data_full.csv\", index=False)\n",
    "    mlflow.log_artifact(\"../data/data_full.csv\")\n",
    "    \n",
    "    # 2. Log feature names\n",
    "    feature_names = pd.DataFrame({'feature_name': X.columns.tolist()})\n",
    "    feature_names.to_csv(\"../data/feature_names.csv\", index=False)\n",
    "    mlflow.log_artifact(\"../data/feature_names.csv\")\n",
    "    \n",
    "    # 3. Log meta features\n",
    "    meta_features_df.to_csv(\"../data/meta_features.csv\", index=False)\n",
    "    mlflow.log_artifact(\"../data/meta_features.csv\")\n",
    "    \n",
    "    # 4. Log Optuna study results for all models\n",
    "    optuna_results_meta = study.trials_dataframe()\n",
    "    optuna_results_meta['model'] = 'meta_learner'\n",
    "    optuna_results_xgb = study_xgb.trials_dataframe()\n",
    "    optuna_results_xgb['model'] = 'xgb'\n",
    "    optuna_results_lgbm = study_lgbm.trials_dataframe()\n",
    "    optuna_results_lgbm['model'] = 'lgbm'\n",
    "    optuna_results_catboost = study_catboost.trials_dataframe()\n",
    "    optuna_results_catboost['model'] = 'catboost'\n",
    "    \n",
    "    all_optuna_results = pd.concat([\n",
    "        optuna_results_xgb, optuna_results_lgbm, \n",
    "        optuna_results_catboost, optuna_results_meta\n",
    "    ], ignore_index=True)\n",
    "    all_optuna_results.to_csv(\"../data/optuna_results.csv\", index=False)\n",
    "    mlflow.log_artifact(\"../data/optuna_results.csv\")\n",
    "    \n",
    "    # 5. Log config/threshold info\n",
    "    config_info = {\n",
    "        'threshold': best_threshold,\n",
    "        'n_folds': N_FOLDS,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'use_smote': USE_SMOTE,\n",
    "        'meta_learner_params': meta_learner_params,\n",
    "        'xgb_params': best_xgb_params,\n",
    "        'lgbm_params': best_lgbm_params,\n",
    "        'catboost_params': best_catboost_params,\n",
    "        'base_models': list(base_models.keys()),\n",
    "    }\n",
    "    pd.DataFrame([config_info]).to_json(\"../data/config.json\", orient='records', indent=2)\n",
    "    mlflow.log_artifact(\"../data/config.json\")\n",
    "    \n",
    "    # ========== LOG MODELS ==========\n",
    "    \n",
    "    # Log meta learner v·ªõi signature\n",
    "    signature = infer_signature(final_meta_features, final_meta_learner.predict(final_meta_features))\n",
    "    mlflow.sklearn.log_model(\n",
    "        final_meta_learner, \n",
    "        \"meta_learner_final\",\n",
    "        signature=signature\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ All models and artifacts logged successfully!\")\n",
    "    print(\"   Saved to ../data/:\")\n",
    "    print(\"   - data_full.csv\")\n",
    "    print(\"   - feature_names.csv\") \n",
    "    print(\"   - meta_features.csv\")\n",
    "    print(\"   - optuna_results.csv (all 4 models)\")\n",
    "    print(\"   - config.json\")\n",
    "    print(\"   MLflow models:\")\n",
    "    print(\"   - base_model_xgb, base_model_lgbm, base_model_catboost\")\n",
    "    print(\"   - meta_learner_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccf32711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "THRESHOLD COMPARISON\n",
      "============================================================\n",
      "\n",
      "Threshold = 0.2: ‚Üê BEST (Optuna)\n",
      "  Precision: 0.4598\n",
      "  Recall:    0.8012\n",
      "  F1:        0.5843\n",
      "  F2:        0.6976\n",
      "  PR-AUC:    0.5858\n",
      "\n",
      "Threshold = 0.3:\n",
      "  Precision: 0.5164\n",
      "  Recall:    0.6815\n",
      "  F1:        0.5876\n",
      "  F2:        0.6405\n",
      "  PR-AUC:    0.5858\n",
      "\n",
      "Threshold = 0.4:\n",
      "  Precision: 0.5636\n",
      "  Recall:    0.5728\n",
      "  F1:        0.5682\n",
      "  F2:        0.5709\n",
      "  PR-AUC:    0.5858\n",
      "\n",
      "Threshold = 0.5:\n",
      "  Precision: 0.6124\n",
      "  Recall:    0.4387\n",
      "  F1:        0.5112\n",
      "  F2:        0.4651\n",
      "  PR-AUC:    0.5858\n",
      "\n",
      "Threshold = 0.6:\n",
      "  Precision: 0.6675\n",
      "  Recall:    0.2983\n",
      "  F1:        0.4123\n",
      "  F2:        0.3354\n",
      "  PR-AUC:    0.5858\n",
      "\n",
      "Threshold = 0.7:\n",
      "  Precision: 0.7439\n",
      "  Recall:    0.1396\n",
      "  F1:        0.2350\n",
      "  F2:        0.1666\n",
      "  PR-AUC:    0.5858\n",
      "\n",
      "============================================================\n",
      "SUMMARY TABLE\n",
      "============================================================\n",
      " threshold  precision   recall       f1       f2   pr_auc\n",
      "       0.2   0.459790 0.801243 0.584288 0.697627 0.585776\n",
      "       0.3   0.516439 0.681492 0.587595 0.640548 0.585776\n",
      "       0.4   0.563611 0.572800 0.568168 0.570938 0.585776\n",
      "       0.5   0.612368 0.438697 0.511185 0.465077 0.585776\n",
      "       0.6   0.667548 0.298302 0.412343 0.335407 0.585776\n",
      "       0.7   0.743862 0.139572 0.235043 0.166648 0.585776\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Compare thresholds\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"THRESHOLD COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "thresholds_to_compare = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "\n",
    "results = []\n",
    "for thresh in thresholds_to_compare:\n",
    "    proba = final_meta_learner.predict_proba(final_meta_features)[:, 1]\n",
    "    pred = (proba >= thresh).astype(int)\n",
    "    metrics = calculate_metrics(y, pred, proba)\n",
    "    metrics['f2'] = fbeta_score(y, pred, beta=2)\n",
    "    metrics['threshold'] = thresh\n",
    "    results.append(metrics)\n",
    "    \n",
    "    marker = \" ‚Üê BEST (Optuna)\" if abs(thresh - best_threshold) < 0.05 else \"\"\n",
    "    print(f\"\\nThreshold = {thresh}:{marker}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1:        {metrics['f1']:.4f}\")\n",
    "    print(f\"  F2:        {metrics['f2']:.4f}\")\n",
    "    print(f\"  PR-AUC:    {metrics['pr_auc']:.4f}\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\" * 60)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df[['threshold', 'precision', 'recall', 'f1', 'f2', 'pr_auc']]\n",
    "print(results_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diabetes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
